{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZZ9oNMdoSmS",
        "outputId": "8636dcaf-05d7-454a-d7fd-05e2a884b653"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "\n",
        "## resource:https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
        "\n",
        "## Create stemmer & Lemmatizer\n",
        "stemmer=PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVumM-OQoSmW"
      },
      "source": [
        "### Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIEs21hwoSmX",
        "outputId": "8313a7a1-0c42-4119-c49c-4a590b075b06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stemming amusing : amus\n",
            "lemmatization amusing : amuse\n"
          ]
        }
      ],
      "source": [
        "print('Stemming amusing : {}'.format(stemmer.stem('amusing')))\n",
        "print('lemmatization amusing : {}'.format(lemmatizer.lemmatize('amusing',pos = 'v')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJB-UnRToSmZ"
      },
      "source": [
        "### Use tokenize + stemming to obtain the root of every word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYXSBLEm0Uxk",
        "outputId": "0d46a2a9-3ee1-4c6a-9868-e13a83cc0384"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIpzX0T2oSmZ",
        "outputId": "2206aef6-a05f-44ee-c133-677864ec67c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['We', 'went', 'out', 'often', ',', 'hiding', 'from', 'sight', ',', 'desperately', 'searching', 'for', 'food', '.']\n",
            "We went out often , hiding from sight , desperately searching for food .\n"
          ]
        }
      ],
      "source": [
        "# Define the sentence to be lemmatized\n",
        "sentence = \"We went out often, hiding from sight, desperately searching for food.\"\n",
        "\n",
        "# Tokenize: Split the sentence into words\n",
        "word_list = nltk.word_tokenize(sentence)\n",
        "print(word_list)\n",
        "#> ['We', 'went', 'out', 'often', ',', 'hiding', 'from', 'sight', ',', 'desperately', 'searching', 'for', 'food', '.']\n",
        "\n",
        "stemming_output = ' '.join([w for w in word_list])\n",
        "print(stemming_output)\n",
        "#> We went out often , hide from sight , desper search for food.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKWIxvj9oSma"
      },
      "source": [
        "### Use tokenize + lemmatize to obtain the lemma of every word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l_Wr4p2oSma",
        "outputId": "eb61aee0-4a9c-41bc-d577-2ecc6625cce7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['We', 'went', 'out', 'often', ',', 'hiding', 'from', 'sight', ',', 'desperately', 'searching', 'for', 'food', '.']\n",
            "We went out often , hiding from sight , desperately searching for food .\n"
          ]
        }
      ],
      "source": [
        "# Define the sentence to be lemmatized\n",
        "sentence = \"We went out often, hiding from sight, desperately searching for food.\"\n",
        "\n",
        "# Tokenize: Split the sentence into words\n",
        "word_list = nltk.word_tokenize(sentence)\n",
        "print(word_list)\n",
        "#> ['We', 'went', 'out', 'often', ',', 'hiding', 'from', 'sight', ',', 'desperately', 'searching', 'for', 'food', '.']\n",
        "\n",
        "# Lemmatize list of words and join\n",
        "lemmatized_output = ' '.join([w for w in word_list])\n",
        "print(lemmatized_output)\n",
        "#> We went out often , hiding from sight , desperately searching for food ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FtjjhaSoSmb"
      },
      "source": [
        "### Sometimes the lemma of a word might change depending to it's Part Of Speech."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4VgbsCzoSmb",
        "outputId": "2b296614-61e2-42a0-9e61-58ea8ed5dd4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lemmatization amusing : amuse\n",
            "lemmatization amusing : amusing\n"
          ]
        }
      ],
      "source": [
        "print('lemmatization amusing : {}'.format(lemmatizer.lemmatize('amusing',pos = 'v'))) ## Verb\n",
        "print('lemmatization amusing : {}'.format(lemmatizer.lemmatize('amusing',pos = 'a'))) ## Adj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWYQncr1oSmc"
      },
      "source": [
        "### Use pos_tag + lemmatize to obtain the lemma of every word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Mh-STfO0cex",
        "outputId": "b7191162-256b-42e1-bd1d-8c627ba06e08"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IKs4YsjIoSmc"
      },
      "outputs": [],
      "source": [
        "# Lemmatize with POS Tag\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"map the pos_tag result to the pos format of the lemmatizer.\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3_kwXkfoSmc",
        "outputId": "ce6a7054-76f2-442d-807e-4c2ceb421f74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "use\n"
          ]
        }
      ],
      "source": [
        "word = 'using'\n",
        "print(lemmatizer.lemmatize(word, get_wordnet_pos(word)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVf9aLOwoSmd"
      },
      "source": [
        "### Lemmatize every word in the sentence then add POS tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfVdfZFioSmd",
        "outputId": "9d58b302-7680-4251-c922-f89ffca87d3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['We', 'go', 'out', 'often', ',', 'hiding', 'from', 'sight', ',', 'desperately', 'search', 'for', 'food', '.']\n"
          ]
        }
      ],
      "source": [
        "sentence = \"We went out often, hiding from sight, desperately searching for food.\"\n",
        "word_list = nltk.word_tokenize(sentence)\n",
        "print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in word_list])\n",
        "\n",
        "#> ['We', 'go', 'out', 'often', ',', 'hiding', 'from', 'sight', ',', 'desperately', 'search', 'for', 'food', '.']\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Stemming + Lemmatise.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.0 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "881621f52f5e994ae267cd7db937900eaa9d03f22234e22f8521ed97ad90d703"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
